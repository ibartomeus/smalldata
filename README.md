# smalldata
*Package documenting the creation of a small data package on bee traits*

This is a guide to create a package to host, manage and deliver small dynamic data to the world. Tinny datasets, often static, containing scientific data can be hosted in several places (e.g. dryad, figshare). Similarly, big projects generating big data have the power to create customized data portals (e.g. gbif). But there is a lot of medium sized data that its well curated, dynamic, but that usually lives in a local computer because is not technically easy and/or cheap to put in the public domain. This is an attempt to liberate such data in a programatic way and use it as a guide or template for others to do the same.

Why small data? Becasue I need a ligth protable and simple data structure: _csv_. This has several advantadges, but the drawback is that data can't be really large (e.g. no more than ~50 MB to work smoothly with git and R).

I will cover the following steps:

1) Host the data: git or dat  
2) Define the metadata   
4) Automate a local check on the data (e.g. using taxize)  
3) Create a R package to interact with the data.  
    - download data  
    - upload data (in a secure way)  
    - use predefined functions (e.g. processed data) on the data  
    - get the correct citation for the used data.  
5) Issues with speed. Partition large files & automate pre-processed data.  
6) Ensure long term permanent repository of the data in different realeases.  
7) Do a shinny app to visualize the data  

The example data will be about bee traits. Actually there is no comprensive database of bee traits, but information is scatered in diverse publications and private databases. However, most importantly none of this data to my knowledge capture the variability in traits, but only mean values for especies. I envision two sets of data. 

- Species level data. Taxonomical. Autogenerated via taxize.
- Specimen level data. Morpfological, ecological, life history or physiological data measured on individuals from which to calculate mean and variance (if desired). In terms of datasets, this can be split in traits measured in phisical individuals (e.g. bosy size), and traits inferred from observational data (e.g. flower specialization).

# Process

1) Host the data: git or dat

Initially I will build 'smalldata' using git and Github as the data server. The advantages is that is well stablished and tools are build around it to streamline the process in a cost effective way. The drawback is that is not ideal for tracking changes on data. dat would be a preferred option, but is in early development stage and principally lacks of an easy-to-use free server to host the project. However, migrating the datasets to dat is planned in a second stage of the project.

Raw data will be stored in /data folder in csv. I need to think if an autogenerated .RData complement may be a good idea for speeding the project load, at least for pre-processed data.

In my case three datasets are needed.

- taxonomy data: Species name and taxonomy (autogenerated with taxize)
- Specimen level data: Species name; (variable group); variable name; value; who; id (sex?)
- Observation data: Species name; host; when; where; who; id (sex?)

2) Define the metadata 

Package EML will be used: https://github.com/ropensci/EML

4) Automate a local check on the data (e.g. using taxize)

I decide only fully identified species will be allowed. Internal checks should check for taxonomy, variable categories, units and ranges, and provide warnings. Internal chacks should be a function that can be called from the package.

3) Create a R package to interact with the data.
    - download raw data: load() and read.table?
    - use predefined functions (e.g. processed data) on the data
    - upload data. This can be done by automating the fork, add & pull request from a single function.
    - get the correct citation for the used data. (including how acknowledgments should look like)

5) Issues with speed. Partition large files & automate pre-processed data.

large files can be partitioned geographically and downloaded only if needed. 
Pre-procesed data (e.g. mean values) can be saved in Rdata file and updated monthly(?). An update function should be available.

6) Ensure long term permanent repository of the data in different realeases.

7) Do a shinny app to visualize the data, specially data coverage (maps).






